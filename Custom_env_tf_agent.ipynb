{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d015dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tiger_cheerag/DQN'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d1be94",
   "metadata": {},
   "source": [
    "`Environment` is the surrounding or setting where the agent performs actions. The agent interacts with the environment and the state of the environment changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56c320",
   "metadata": {},
   "source": [
    "## *Custom Environment for Tic-tac-toe*\n",
    "\n",
    "Instead of two players, the simplified Tic-tac-toe has only `one player`. The player chooses positions randomly and if the position s/he chooses has already been chosen, the game ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67d953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aca0ab",
   "metadata": {},
   "source": [
    "Environments can be either Python environment or TensorFlow environment. Python environments are simple to implement but TensorFlow environments are more efficient\n",
    "\n",
    "Python environment and use one of our wrappers to automatically convert it to the TensorFlow environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d77ce43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedTicTacToe(py_environment.PyEnvironment):\n",
    "    '''\n",
    "    action_spec: describes the specifications (TensorSpecs) of the action expected by step\n",
    "    \n",
    "    observation_spec: defines the specifications (TensorSpec) of observations provided by the environment\n",
    "    \n",
    "    _reset: returns the current situation (TimeStep) after resetting the environment\n",
    "    \n",
    "    _step: applies the action and returns the new situation (TimeStep)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=8, name='play')\n",
    "        \n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,9), dtype=np.int32, minimum=0, maximum=1, name='board')\n",
    "        \n",
    "        self._state = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        self._episode_ended = False\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "    \n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def _reset(self):\n",
    "          # state at the start of the game\n",
    "        self._state = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "    #checking if some spot is empty and if all the spots are occupied.\n",
    "    def __is_spot_empty(self, ind):\n",
    "        return self._state[ind] == 0\n",
    "\n",
    "    def __all_spots_occupied(self):\n",
    "        return all(i == 1 for i in self._state)\n",
    "    \n",
    "    \n",
    "    def _step(self, action): \n",
    "        '''\n",
    "        It applies the action and returns the new situation in the game.\n",
    "        The situation is of the class TimeStep in TensorFlow.\n",
    "        TimeStep(step_type, reward, discount, observation)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            return self._reset()\n",
    "\n",
    "        if self.__is_spot_empty(action):        \n",
    "            self._state[action] = 1\n",
    "\n",
    "            if self.__all_spots_occupied():\n",
    "                self._episode_ended = True\n",
    "                return ts.termination(np.array([self._state], dtype=np.int32), 1)\n",
    "            else:\n",
    "                return ts.transition(np.array([self._state], dtype=np.int32), reward=0.05, discount=1.0)\n",
    "\n",
    "        else:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(np.array([self._state], dtype=np.int32), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd385d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_environment = SimplifiedTicTacToe()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(python_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e9081b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "number_of_episodes = 10000\n",
    "\n",
    "for _ in range(number_of_episodes):\n",
    "    episode_steps = 0\n",
    "    episode_reward = 0\n",
    "    tf_env.reset()\n",
    "    while True:\n",
    "        action = tf.random.uniform([1], 0, 9, dtype=tf.int32)\n",
    "        next_time_step = tf_env.step(action)\n",
    "        if tf_env.current_time_step().is_last():\n",
    "            break\n",
    "        episode_steps += 1\n",
    "        episode_reward += next_time_step.reward.numpy()\n",
    "    #print(episode_steps)    \n",
    "    rewards.append(episode_reward)\n",
    "    steps.append(episode_steps)\n",
    "    \n",
    "# for _ in range(num_episodes):\n",
    "#     episode_reward = 0\n",
    "#     episode_steps = 0\n",
    "#     tf_env.reset()\n",
    "#     while not tf_env.current_time_step().is_last():\n",
    "#         action = tf.random_uniform([1], 0, 9, dtype=tf.int32)\n",
    "#         next_time_step = tf_env.step(action)\n",
    "#         episode_steps += 1\n",
    "#         episode_reward += next_time_step.reward.numpy()\n",
    "#     rewards.append(episode_reward)\n",
    "#     steps.append(episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "054354e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4569"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_no_of_steps = np.mean(steps)\n",
    "mean_no_of_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167a0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
